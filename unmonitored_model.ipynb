{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/site-packages (1.3.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in /usr/local/lib/python3.10/site-packages (from scikit-learn) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/site-packages (from scikit-learn) (1.11.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/site-packages (from scikit-learn) (3.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/site-packages (from matplotlib) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/site-packages (from matplotlib) (4.43.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in /usr/local/lib/python3.10/site-packages (from matplotlib) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/heeeee/Library/Python/3.10/lib/python/site-packages (from matplotlib) (23.1)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/site-packages (from matplotlib) (10.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/heeeee/Library/Python/3.10/lib/python/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/site-packages (from cycler>=0.10->matplotlib) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install scikit-learn\n",
    "%pip install matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "5mfwrTwPtd36"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4L0A001wudOX",
    "outputId": "f367e1fa-c59b-476a-99ce-ddb149cac720"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datafile...\n"
     ]
    }
   ],
   "source": [
    "# Load the pickle file\n",
    "print(\"Loading datafile...\")\n",
    "## 경로 바꾸기\n",
    "with open(\"./mon_standard.pkl\", 'rb') as fi: # Path to mon_standard.pkl in local\n",
    "    mon_data = pickle.load(fi)\n",
    "\n",
    "with open('./unmon_standard10.pkl', 'rb') as f1:  # Path to unmon_standard10.pkl in local\n",
    "    unmon_data = pickle.load(f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z1XOJHf6qBXl",
    "outputId": "b992e1ab-bd4e-4365-abd8-38137e36f37e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Mon samples: 19000\n",
      "Total Unmon samples: 10000\n"
     ]
    }
   ],
   "source": [
    "USE_SUBLABEL = False\n",
    "URL_PER_SITE = 10\n",
    "TOTAL_URLS   = 950\n",
    "\n",
    "mon_X1 = [] # Array to store instances (timestamps) - 19,000 instances, e.g., [[0.0, 0.5, 3.4, ...], [0.0, 4.5, ...], [0.0, 1.5, ...], ... [... ,45.8]]\n",
    "mon_X2 = [] # Array to store instances (direction*size) - size information\n",
    "mon_y = [] # Array to store the site of each instance - 19,000 instances, e.g., [0, 0, 0, 0, 0, 0, ..., 94, 94, 94, 94, 94]\n",
    "\n",
    "\n",
    "# Differentiate instances and sites, and store them in the respective x and y arrays\n",
    "# x array (direction*timestamp), y array (site label)\n",
    "for i in range(TOTAL_URLS):\n",
    "    if USE_SUBLABEL:\n",
    "        label = i\n",
    "    else:\n",
    "        label = i // URL_PER_SITE # Calculate which site's URL the current URL being processed belongs to and set that value as the label. Thus, URLs fetched from the same site are labeled identically.\n",
    "    for sample in mon_data[i]:\n",
    "        size_seq = []\n",
    "        time_seq = []\n",
    "        for c in sample:\n",
    "            dr = 1 if c > 0 else -1\n",
    "            time_seq.append(abs(c))\n",
    "            size_seq.append(dr * 512)\n",
    "        # print(len(time_seq))\n",
    "        mon_X1.append(time_seq)\n",
    "        mon_X2.append(size_seq)\n",
    "        mon_y.append(label)\n",
    "mon_size = len(mon_y)\n",
    "\n",
    "print(f'Total Mon samples: {mon_size}') # Output: 19000\n",
    "\n",
    "UNMON_TOTAL_URLS = 10000\n",
    "unmon_X1 = [] # Array to store instances (timestamps) - 10,000 instances, e.g., [[0.0, 0.5, 3.4, ...], [0.0, 4.5, ...], [0.0, 1.5, ...], ... [... ,45.8]]\n",
    "unmon_X2 = [] # Array to store instances (direction*size) - size information\n",
    "\n",
    "for i in range(UNMON_TOTAL_URLS):\n",
    "    size_seq = []\n",
    "    time_seq = []\n",
    "    for c in unmon_data[i]:\n",
    "        dr = 1 if c > 0 else -1\n",
    "        time_seq.append(abs(c))\n",
    "        size_seq.append(dr * 512) # In the pickle file, there is no size information, so the conversion code is set to multiply by 512 uniformly.\n",
    "    unmon_X1.append(time_seq)\n",
    "    unmon_X2.append(size_seq)\n",
    "unmon_size = len(unmon_X1)\n",
    "unmon_y = [ 95 for sample_idx in range(unmon_size)]\n",
    "print(f'Total Unmon samples: {unmon_size}') # Output: 19000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "O47oVisaZhWk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data X1 feature shape: (29000,)\n",
      "Total data X2 shape: (29000,)\n",
      "Total data y_multi shape: (29000,)\n",
      "Total data y_binary shape: (29000,)\n"
     ]
    }
   ],
   "source": [
    "X1 = mon_X1 + unmon_X1 \n",
    "X1 = np.array(X1,dtype=object)\n",
    "X2 = mon_X2 + unmon_X2\n",
    "X2 = np.array(X2,dtype=object)\n",
    "# feature기반으로 test data생성할 때 넣는 y값이 다름\n",
    "y_multi = mon_y + unmon_y\n",
    "y_binary = [1 for sample_idx in range(mon_size)] + unmon_y\n",
    "y_multi = np.array(y_multi, dtype=object)\n",
    "y_binary = np.array(y_binary, dtype=object)\n",
    "\n",
    "\n",
    "print(f\"Total data X1 feature shape: {X1.shape}\")\n",
    "print(f\"Total data X2 shape: {X2.shape}\")\n",
    "print(f\"Total data y_multi shape: {y_multi.shape}\")\n",
    "print(f\"Total data y_binary shape: {y_binary.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7rFZSE7huKZn",
    "outputId": "cfbcfef0-2336-443e-82a2-f552be3d1fdc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. feature shape : (29000, 1)\n",
      "2. feature shape : (29000, 1)\n",
      "3. feature shape : (29000, 1)\n",
      "4. feature shape : (29000, 1)\n",
      "5. feature shape : (29000, 1)\n",
      "6. feature shape : (29000, 1)\n",
      "7. feature shape : (29000, 1)\n",
      "8. feature shape : (29000, 1)\n",
      "9. feature shape : (29000, 1)\n",
      "10. feature shape : (29000, 1)\n",
      "11. feature shape : (29000, 1)\n",
      "12. feature shape : (29000, 1)\n"
     ]
    }
   ],
   "source": [
    "# 1. number of incoming packets\n",
    "incoming_packets = [np.sum(np.array(sub_array) < 0) for sub_array in X2]\n",
    "incoming_packets = np.array(incoming_packets).reshape(-1, 1)\n",
    "\n",
    "# 2. the total number of incoming packets stats in first 30 packets\n",
    "incoming_packets_in_first_30_packets = [np.sum(np.array(sub_array[:30]) < 0) for sub_array in X2]\n",
    "incoming_packets_in_first_30_packets = np.array(incoming_packets_in_first_30_packets).reshape(-1, 1)\n",
    "\n",
    "# 3. the total number of outcoming packets stats in first 30 packets\n",
    "outgoing_packets_in_first_30_packets = [np.sum(np.array(sub_array[:30]) > 0) for sub_array in X2]\n",
    "outgoing_packets_in_first_30_packets = np.array(outgoing_packets_in_first_30_packets).reshape(-1, 1)\n",
    "\n",
    "# 4. number of outgoing packets as a fraction of the total number of packets\n",
    "outgoing_fraction = [np.sum(np.array(sub_array) > 0) / len(sub_array) if len(sub_array) != 0 else 0 for sub_array in X2]\n",
    "outgoing_fraction = np.array(outgoing_fraction).reshape(-1, 1)\n",
    "\n",
    "# 5. total number of packets\n",
    "total_packets_count = [len(sub_array) for sub_array in X2]\n",
    "total_packets_count = np.array(total_packets_count)\n",
    "total_packets_count_2D = total_packets_count.reshape(-1, 1)\n",
    "\n",
    "# 6. Compute fraction of incoming packets for each entry in X2\n",
    "incoming_fraction = [np.sum(np.array(sub_array) < 0) / len(sub_array) if len(sub_array) != 0 else 0 for sub_array in X2]\n",
    "incoming_fraction = np.array(incoming_fraction).reshape(-1, 1)\n",
    "\n",
    "# 7. number of outgoing packets\n",
    "outgoing_packets = [np.sum(np.array(sub_array) > 0) for sub_array in X2]\n",
    "outgoing_packets = np.array(outgoing_packets).reshape(-1, 1)\n",
    "\n",
    "# 8. standard deviation of the outgoing packet ordering list\n",
    "std_deviation_outgoing = [np.std(sub_array) for sub_array in X2]\n",
    "std_deviation_outgoing = np.array(std_deviation_outgoing).reshape(-1, 1)\n",
    "\n",
    "# 9. \n",
    "avg_outgoing_order = []\n",
    "for time_seq, size_seq in zip(X1, X2):\n",
    "    outgoing_times = [t for t, s in zip(time_seq, size_seq) if s > 0]\n",
    "    avg_outgoing_order.append(np.mean(outgoing_times) if outgoing_times else 0)\n",
    "avg_outgoing_order = np.array(avg_outgoing_order).reshape(-1, 1)\n",
    "\n",
    "\n",
    "# 10.\n",
    "incoming_counts = []\n",
    "outgoing_counts = []\n",
    "total_counts = []\n",
    "combined_counts = []\n",
    "\n",
    "for size_seq in X2:\n",
    "    incoming_counts.append(np.sum(np.array(size_seq) < 0))\n",
    "    outgoing_counts.append(np.sum(np.array(size_seq) > 0))\n",
    "    total_counts.append(len(size_seq))\n",
    "total_counts = np.array(total_counts).reshape(-1, 1)\n",
    "\n",
    "# 11. Sum of incoming, outgoing and total number of packets\n",
    "for i in range(len(total_counts)):\n",
    "    combined_count = incoming_counts[i] + outgoing_counts[i] + total_counts[i]\n",
    "    combined_counts.append(combined_count)\n",
    "combined_counts = np.array(combined_counts).reshape(-1, 1)\n",
    "\n",
    "\n",
    "# 12.\n",
    "packet_concentration = []\n",
    "\n",
    "for time_seq in X1:\n",
    "    # Packet Concentration: Calculate time differences between packets\n",
    "    if len(time_seq) > 1:\n",
    "        time_diffs = np.diff(time_seq)\n",
    "        concentration_feature = np.mean(time_diffs)  # 평균 시간 간격\n",
    "    else:\n",
    "        concentration_feature = 0  # 패킷이 하나 뿐인 경우\n",
    "\n",
    "    packet_concentration.append(concentration_feature)\n",
    "packet_concentration = np.array(packet_concentration).reshape(-1, 1)\n",
    "\n",
    "print(f\"1. feature shape : {incoming_packets.shape}\")\n",
    "print(f\"2. feature shape : {incoming_packets_in_first_30_packets.shape}\")\n",
    "print(f\"3. feature shape : {outgoing_packets_in_first_30_packets.shape}\")\n",
    "print(f\"4. feature shape : {outgoing_fraction.shape}\")\n",
    "print(f\"5. feature shape : {total_packets_count_2D.shape}\")\n",
    "print(f\"6. feature shape : {incoming_fraction.shape}\")\n",
    "print(f\"7. feature shape : {outgoing_packets.shape}\")\n",
    "print(f\"8. feature shape : {std_deviation_outgoing.shape}\")\n",
    "print(f\"9. feature shape : {avg_outgoing_order.shape}\")\n",
    "print(f\"10. feature shape : {total_counts.shape}\")\n",
    "print(f\"11. feature shape : {combined_counts.shape}\")\n",
    "print(f\"12. feature shape : {packet_concentration.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_combined = np.hstack((\n",
    "    incoming_packets,\n",
    "    incoming_packets_in_first_30_packets,\n",
    "    outgoing_packets_in_first_30_packets,\n",
    "    outgoing_fraction,\n",
    "    total_packets_count_2D,\n",
    "    incoming_fraction,\n",
    "    outgoing_packets,\n",
    "    std_deviation_outgoing,\n",
    "    avg_outgoing_order,\n",
    "    total_counts,\n",
    "    combined_counts,\n",
    "    packet_concentration\n",
    ")).astype(np.float64) \n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "# for binary test and multi test\n",
    "y_binary_test = label_encoder.fit_transform(y_binary)\n",
    "y_multi_test = label_encoder.fit_transform(y_multi)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open World - binary class\n",
    "## Naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7260\n"
     ]
    }
   ],
   "source": [
    "# Assuming X_categorical and y are your features and labels respectively\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y_binary_test, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize the Gaussian Naive Bayes classifier\n",
    "gnb = GaussianNB()\n",
    "\n",
    "# Train the classifier\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = gnb.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Generate a classification report\n",
    "#print(\"\\nClassification Report:\\n\")\n",
    "#print(classification_report(y_test, y_pred, zero_division=0))\n",
    "\n",
    "# Confusion Matrix\n",
    "#conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plotting the Confusion Matrix\n",
    "#plt.figure(figsize=(10, 8))\n",
    "#sns.heatmap(conf_matrix, annot=True, fmt='g')\n",
    "#plt.title('Confusion Matrix')\n",
    "#plt.ylabel('Actual Labels')\n",
    "#plt.xlabel('Predicted Labels')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6574\n"
     ]
    }
   ],
   "source": [
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y_binary_test, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initializing the Support Vector Machine (SVM) model\n",
    "svm_model = SVC()\n",
    "\n",
    "# Training the model with the training data\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions on the test data\n",
    "y_pred = svm_model.predict(X_test)\n",
    "\n",
    "# Calculating the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open World - multi class\n",
    "## Naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.1505\n"
     ]
    }
   ],
   "source": [
    "# Assuming X_categorical and y are your features and labels respectively\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y_multi_test, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize the Gaussian Naive Bayes classifier\n",
    "gnb = GaussianNB()\n",
    "\n",
    "# Train the classifier\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = gnb.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3426\n"
     ]
    }
   ],
   "source": [
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y_multi_test, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initializing the Support Vector Machine (SVM) model\n",
    "svm_model = SVC()\n",
    "\n",
    "# Training the model with the training data\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions on the test data\n",
    "y_pred = svm_model.predict(X_test)\n",
    "\n",
    "# Calculating the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper parameter Tuning SVM 1st trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n",
      "[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time= 2.6min\n",
      "[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time= 2.6min\n",
      "[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time= 2.6min\n",
      "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time= 2.6min\n",
      "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time= 2.6min\n",
      "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time= 2.7min\n",
      "[CV] END .........................C=1, gamma=1.0, kernel=rbf; total time= 2.6min\n",
      "[CV] END .........................C=1, gamma=1.0, kernel=rbf; total time= 2.6min\n",
      "[CV] END .........................C=1, gamma=1.0, kernel=rbf; total time= 2.5min\n",
      "[CV] END .......................C=10, gamma=0.01, kernel=rbf; total time= 2.6min\n",
      "[CV] END .......................C=10, gamma=0.01, kernel=rbf; total time= 2.7min\n",
      "[CV] END .......................C=10, gamma=0.01, kernel=rbf; total time= 2.7min\n",
      "[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time= 2.7min\n",
      "[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time= 2.7min\n",
      "[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time= 2.7min\n",
      "[CV] END ........................C=10, gamma=1.0, kernel=rbf; total time= 2.7min\n",
      "[CV] END ........................C=10, gamma=1.0, kernel=rbf; total time= 2.7min\n",
      "[CV] END ........................C=10, gamma=1.0, kernel=rbf; total time= 2.7min\n",
      "[CV] END ......................C=100, gamma=0.01, kernel=rbf; total time= 2.7min\n",
      "[CV] END ......................C=100, gamma=0.01, kernel=rbf; total time= 2.7min\n",
      "[CV] END ......................C=100, gamma=0.01, kernel=rbf; total time= 2.7min\n",
      "[CV] END .......................C=100, gamma=0.1, kernel=rbf; total time= 2.7min\n",
      "[CV] END .......................C=100, gamma=0.1, kernel=rbf; total time= 2.7min\n",
      "[CV] END .......................C=100, gamma=0.1, kernel=rbf; total time= 2.7min\n",
      "[CV] END .......................C=100, gamma=1.0, kernel=rbf; total time= 2.7min\n",
      "[CV] END .......................C=100, gamma=1.0, kernel=rbf; total time= 2.7min\n",
      "[CV] END .......................C=100, gamma=1.0, kernel=rbf; total time= 2.7min\n",
      "Best Hyperparameters: {'C': 10, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "Accuracy: 0.4361\n"
     ]
    }
   ],
   "source": [
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y_multi_test, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initializing the Support Vector Machine (SVM) model\n",
    "svm_model = SVC(kernel='rbf', decision_function_shape='ovo')\n",
    "\n",
    "# 하이퍼파라미터 범위 축소\n",
    "param_grid = {\n",
    "    'C': [1, 10, 100],\n",
    "    'gamma': [0.01, 0.1, 1.0],\n",
    "    'kernel': ['rbf']\n",
    "}\n",
    "\n",
    "# 랜덤 서치 초기화\n",
    "random_search = GridSearchCV(SVC(decision_function_shape='ovo'), param_grid, cv=3, scoring='accuracy', verbose=2)\n",
    "\n",
    "# 모델 훈련\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# 최적의 하이퍼파라미터 출력\n",
    "print(\"Best Hyperparameters:\", random_search.best_params_)\n",
    "\n",
    "# 최적의 모델로 예측\n",
    "best_svm_model = random_search.best_estimator_\n",
    "y_pred = best_svm_model.predict(X_test)\n",
    "\n",
    "# 정확도 계산\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper parameter Tuning SVM 2nd trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "[CV] END .......................C=100, gamma=0.1, kernel=rbf; total time= 2.7min\n",
      "[CV] END .......................C=100, gamma=0.1, kernel=rbf; total time= 2.6min\n",
      "[CV] END .......................C=100, gamma=0.1, kernel=rbf; total time= 2.7min\n",
      "[CV] END .......................C=200, gamma=0.1, kernel=rbf; total time= 2.7min\n",
      "[CV] END .......................C=200, gamma=0.1, kernel=rbf; total time= 2.7min\n",
      "[CV] END .......................C=200, gamma=0.1, kernel=rbf; total time= 2.7min\n",
      "[CV] END .......................C=300, gamma=0.1, kernel=rbf; total time= 2.7min\n",
      "[CV] END .......................C=300, gamma=0.1, kernel=rbf; total time= 2.7min\n",
      "[CV] END .......................C=300, gamma=0.1, kernel=rbf; total time= 2.8min\n",
      "Best Hyperparameters: {'C': 100, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "Accuracy: 0.3793\n"
     ]
    }
   ],
   "source": [
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y_multi_test, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initializing the Support Vector Machine (SVM) model\n",
    "svm_model = SVC(kernel='rbf', decision_function_shape='ovo')\n",
    "\n",
    "param_grid = {\n",
    "    'C': [100, 200, 300],\n",
    "    'gamma': [0.1],\n",
    "    'kernel': ['rbf']\n",
    "}\n",
    "\n",
    "# 그리드 서치 초기화\n",
    "grid_search = GridSearchCV(SVC(decision_function_shape='ovo'), param_grid, cv=3, scoring='accuracy', verbose=2)\n",
    "\n",
    "# 모델 훈련\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# 최적의 하이퍼파라미터 출력\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "\n",
    "# 최적의 모델로 예측\n",
    "best_svm_model = grid_search.best_estimator_\n",
    "y_pred = best_svm_model.predict(X_test)\n",
    "\n",
    "# 정확도 계산\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper parameter Tuning SVM 3rd trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time= 2.7min\n",
      "[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time= 2.7min\n",
      "[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time= 2.7min\n",
      "[CV] END ........................C=20, gamma=0.1, kernel=rbf; total time= 2.7min\n",
      "[CV] END ........................C=20, gamma=0.1, kernel=rbf; total time= 2.7min\n",
      "[CV] END ........................C=20, gamma=0.1, kernel=rbf; total time= 2.7min\n",
      "[CV] END ........................C=30, gamma=0.1, kernel=rbf; total time= 2.7min\n",
      "[CV] END ........................C=30, gamma=0.1, kernel=rbf; total time= 2.7min\n",
      "[CV] END ........................C=30, gamma=0.1, kernel=rbf; total time= 2.8min\n",
      "Best Hyperparameters: {'C': 10, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "Accuracy: 0.3792\n"
     ]
    }
   ],
   "source": [
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y_multi_test, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initializing the Support Vector Machine (SVM) model\n",
    "svm_model = SVC(kernel='rbf', decision_function_shape='ovo')\n",
    "\n",
    "param_grid = {\n",
    "    'C': [10, 20, 30],\n",
    "    'gamma': [0.1],\n",
    "    'kernel': ['rbf']\n",
    "}\n",
    "\n",
    "# 그리드 서치 초기화\n",
    "grid_search = GridSearchCV(SVC(decision_function_shape='ovo'), param_grid, cv=3, scoring='accuracy', verbose=2)\n",
    "\n",
    "# 모델 훈련\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# 최적의 하이퍼파라미터 출력\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "\n",
    "# 최적의 모델로 예측\n",
    "best_svm_model = grid_search.best_estimator_\n",
    "y_pred = best_svm_model.predict(X_test)\n",
    "\n",
    "# 정확도 계산\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper parameter Tuning SVM 4th trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time= 2.6min\n",
      "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time= 2.7min\n",
      "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time= 2.6min\n",
      "[CV] END .........................C=5, gamma=0.1, kernel=rbf; total time= 2.7min\n",
      "[CV] END .........................C=5, gamma=0.1, kernel=rbf; total time= 2.8min\n",
      "[CV] END .........................C=5, gamma=0.1, kernel=rbf; total time= 2.7min\n",
      "[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time= 2.7min\n",
      "[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time= 2.8min\n",
      "[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time= 2.7min\n",
      "Best Hyperparameters: {'C': 5, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "Accuracy: 0.3790\n"
     ]
    }
   ],
   "source": [
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y_multi_test, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initializing the Support Vector Machine (SVM) model\n",
    "svm_model = SVC(kernel='rbf', decision_function_shape='ovo')\n",
    "\n",
    "param_grid = {\n",
    "    'C': [1, 5, 10],\n",
    "    'gamma': [0.1],\n",
    "    'kernel': ['rbf']\n",
    "}\n",
    "\n",
    "# 그리드 서치 초기화\n",
    "grid_search = GridSearchCV(SVC(decision_function_shape='ovo'), param_grid, cv=3, scoring='accuracy', verbose=2)\n",
    "\n",
    "# 모델 훈련\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# 최적의 하이퍼파라미터 출력\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "\n",
    "# 최적의 모델로 예측\n",
    "best_svm_model = grid_search.best_estimator_\n",
    "y_pred = best_svm_model.predict(X_test)\n",
    "\n",
    "# 정확도 계산\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper parameter Tuning SVM 5th trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "[CV] END .......................C=300, gamma=0.1, kernel=rbf; total time= 2.7min\n",
      "[CV] END .......................C=300, gamma=0.1, kernel=rbf; total time= 2.8min\n",
      "[CV] END .......................C=300, gamma=0.1, kernel=rbf; total time= 2.8min\n",
      "[CV] END .......................C=500, gamma=0.1, kernel=rbf; total time= 2.7min\n",
      "[CV] END .......................C=500, gamma=0.1, kernel=rbf; total time= 2.7min\n",
      "[CV] END .......................C=500, gamma=0.1, kernel=rbf; total time= 2.7min\n",
      "[CV] END ......................C=1000, gamma=0.1, kernel=rbf; total time= 2.7min\n",
      "[CV] END ......................C=1000, gamma=0.1, kernel=rbf; total time= 2.7min\n",
      "[CV] END ......................C=1000, gamma=0.1, kernel=rbf; total time= 2.7min\n",
      "Best Hyperparameters: {'C': 300, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "Accuracy: 0.3793\n"
     ]
    }
   ],
   "source": [
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y_multi_test, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initializing the Support Vector Machine (SVM) model\n",
    "svm_model = SVC(kernel='rbf', decision_function_shape='ovo')\n",
    "\n",
    "param_grid = {\n",
    "    'C': [300, 500, 1000],\n",
    "    'gamma': [0.1],\n",
    "    'kernel': ['rbf']\n",
    "}\n",
    "\n",
    "# 그리드 서치 초기화\n",
    "grid_search = GridSearchCV(SVC(decision_function_shape='ovo'), param_grid, cv=3, scoring='accuracy', verbose=2)\n",
    "\n",
    "# 모델 훈련\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# 최적의 하이퍼파라미터 출력\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "\n",
    "# 최적의 모델로 예측\n",
    "best_svm_model = grid_search.best_estimator_\n",
    "y_pred = best_svm_model.predict(X_test)\n",
    "\n",
    "# 정확도 계산\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
